# Legal One Technical Challenge

This project is aimed at efficiently processing large log files, which can contain terabytes of data generated by various microservices. The challenge was to parse these logs, structure them, and store them in a database for analysis.

## Overview

The solution involves creating a background job that reads the log file in manageable chunks and then inserts the parsed data into a database. Once the data is structured in the database, there is an API endpoint available to retrieve aggregate counts of the logs based on applied filters.

## Setup

To set up the project, follow these steps:

1. Clone this repository to your local machine.

2. Database configuration:
    - Basic database credentials are pre-configured in the `docker-compose.yml` file for a PostgreSQL database.
    - If you need to change the credentials, update them in both the `.env` file and the `docker-compose.yml` file under the `db` service.

3. Build and run the project:
    ```
    docker-compose build
    docker-compose up
    ```
   This command will start the cron job and the web server.

4. Access the OpenAPI UI:
    - Open your browser and go to [http://127.0.0.1:8000/api/doc](http://127.0.0.1:8000/api/doc).
    - Here, you can test the filters and interact with the API using the provided OpenAPI UI.

## Configuration

- **LogFile:**  The sample log file is inside `/var/log/logs.log`. Thats provided in the actual technical challenge. Feel free to replace it with the one you wanted to test it.

- **Chunk Size:** The size of the chunks used to process the log file can be adjusted according to your requirements. You can modify the chunk size from the `job.sh` file by changing the value (`1024` by default) to your preferred size.
    ***Note***: Wanted to do it in env but something crazy was happening in local and docker soooo ¯\\\_(ツ)_/¯.

- **Memory Limit:** PHP has a default memory limit, so if you're dealing with large files or increasing the chunk size significantly, it's advisable to adjust the `memory_limit` setting accordingly.

- **Cron Job Schedule**: The cron job responsible for processing the logs runs every minute. You can configure its schedule inside the `Dockerfile`. Right now, it is set to run every minute:

Dockerfile
```
RUN echo "* * * * * /usr/local/bin/php /var/www/html/bin/console app:process-log >> /var/log/symfony.log 2>&1" > /etc/cron.d/process-log
```

Replace the cron schedule expression (`* * * * *`) with your desired schedule and Viola!.
## Usage

To process the log file, we have this command inside `job.sh` in the root directory. This should triggered automatically via cron service:
```
php /var/www/html/bin/console app:process-log 1024
```
Replace `1024` with your desired chunk size.

If you make any changes to the configuration or codebase and they don't reflect, try rebuilding the container by executing the following command:
```
docker-compose down && docker-compose build && docker-compose up
```

### Note:
- To see the background job logs try this command while the container is up and running
```
docker-compose exec app tail -f /var/log/symfony.log
```


## Testing

To run all unit tests, execute the following command:
```
docker-compose exec app php bin/phpunit
```

## Support

If you have any queries or need further assistance, feel free to reach out to me anytime.

---

This README provides a detailed guide to set up, configure, and run LegalOne Technical Challenge. Follow the instructions carefully to efficiently analyze large log files and extract valuable insights.
